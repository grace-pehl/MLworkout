<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Grace Pehl, PhD by grace-pehl</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Grace Pehl, PhD</h1>
      <h2 class="project-tagline">Can Your Fitbit Tell When You&#39;re Slacking? Classification of Weightlifting Technique from Personal Activity Trackers</h2>
      <a href="http://grace-pehl.github.io/" class="btn">Return to Main Page</a>
      <a href="https://github.com/grace-pehl/MLworkout" class="btn">View on GitHub</a>
      <a href="https://github.com/grace-pehl/MLworkout/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/grace-pehl/MLworkout/tarball/master" class="btn">Download .tar.gz</a>
      <p><a href="https://www.linkedin.com/in/gracepehl"><img src="https://static.licdn.com/scds/common/u/img/webpromo/btn_viewmy_160x33.png" alt="View Grace Pehl's profile on LinkedIn"></a></p>
    </section>

    <section class="main-content">
      <p>Objective: Build a machine learning algorithm to predict activity quality from activity monitors</p>

<p>Course project for Practical Machine Learning, part of the Johns Hopkins Data Science Specialization</p>

<h4>
<a id="data-citation" class="anchor" href="#data-citation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data citation:</h4>

<p>Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. <a href="http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201">Qualitative Activity Recognition of Weight Lifting Exercises</a>. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.</p>
<hr>
<h3>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h3>

<p>The dataset consists of 160 features from accelerometers on the arm, forearm, belt, and dumbbell of 6 participants who performed a series of weightlifting exercises both correctly and incorrectly. The subjects performed a set of 10 repetitions of unilateral dumbbell biceps curls in 5 different ways.  The class A set was done with proper technique. The class B set was done throwing the elbows forward; class C, lifting the dumbbell only halfway; class D, lowering the dumbbell only halfway; and class E, throwing the hips forward. </p>

<p>The complete dataset is split into a training dataset with 13737 observations, a testing dataset with 5885 observations, and a validation set with 20 observations. In the training set, there are no complete cases (observations with no missing values for any feature). Removing 100 features with more than 80% missing values and 7 identification features, leaves 52 features and 13737 out of 13737 complete cases.  The distribution of classes in the training set is: </p>

<table>
<b><tr><td>Class</td><td>Technique</td><td>Counts</td><td>Proportion</td></tr></b>
<tr><td>A</td><td>Proper Form</td><td>3906</td><td>0.28</td></tr>
<tr><td>B</td><td>Elbows Forward</td><td>2658</td><td>0.19</td></tr>
<tr><td>C</td><td>Halfway Up</td><td>2396</td><td>0.17</td></tr>
<tr><td>D</td><td>Halfway Down</td><td>2252</td><td>0.16</td></tr>
<tr><td>E</td><td>Hips Forward</td><td>2525</td><td>0.18</td></tr>
</table>

<h3>
<a id="principal-component-analysis" class="anchor" href="#principal-component-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a>Principal Component Analysis</h3>

<p>In order to reduce the dimensionality of the dataset (the number of features) and to speed computation time, I perform a principal component analysis on the training set.  The preProcess function will center and scale the features prior to PCA and principal components will be calculated to retain 95% of the variance within the data.</p>

<div class="highlight highlight-source-r"><pre><span class="pl-c"># Perform Principal Component Analysis on the training set</span>
<span class="pl-smi">preProc</span> <span class="pl-k">&lt;-</span> preProcess(<span class="pl-smi">training</span>[ , <span class="pl-k">-</span><span class="pl-c1">53</span>], <span class="pl-v">method</span> <span class="pl-k">=</span> <span class="pl-s"><span class="pl-pds">"</span>pca<span class="pl-pds">"</span></span>)
<span class="pl-c"># Apply the transform to all 3 datasets</span>
<span class="pl-smi">trainPC</span> <span class="pl-k">&lt;-</span> predict(<span class="pl-smi">preProc</span>, <span class="pl-smi">training</span>[ , <span class="pl-k">-</span><span class="pl-c1">53</span>])
<span class="pl-smi">testPC</span>  <span class="pl-k">&lt;-</span> predict(<span class="pl-smi">preProc</span>, <span class="pl-smi">testing</span>[ , <span class="pl-k">-</span><span class="pl-c1">53</span>])
<span class="pl-smi">validationPC</span> <span class="pl-k">&lt;-</span> predict(<span class="pl-smi">preProc</span>, <span class="pl-smi">validation</span>[ , <span class="pl-k">-</span><span class="pl-c1">53</span>])</pre></div>

<p>PCA reduces the number of features from 52 to 25.  Using the same components, I also transform the testing and validation sets.  </p>

<div class="highlight highlight-source-r"><pre>suppressMessages(library(<span class="pl-smi">randomForest</span>))
<span class="pl-smi">modFit</span> <span class="pl-k">&lt;-</span> rfcv(<span class="pl-smi">trainPC</span>, <span class="pl-smi">training_labels</span>, <span class="pl-v">step</span> <span class="pl-k">=</span> <span class="pl-c1">0.8</span>, <span class="pl-v">cv.fold</span> <span class="pl-k">=</span> <span class="pl-c1">10</span>)</pre></div>

<h3>
<a id="estimating-out-of-sample-error-with-cross-validation" class="anchor" href="#estimating-out-of-sample-error-with-cross-validation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Estimating Out of Sample Error with Cross Validation</h3>

<p>I use random forest with 10-fold cross validation to select the most important features for the model.  In each fold, I retain the top 80% of features.</p>

<p><img src="https://github.com/grace-pehl/MLworkout/blob/master/analysis_files/figure-html/SelectingFeaturesPlot-1.png?raw=true" alt=""> </p>

<p>The cross validation results show that there is very little difference between using all 26 principal components and using only 11.  With 11 features, we would expect an out-of-sample error rate around 5%.  I use the the top 11 principal components (based on mean Gini decrease) to fit a random forest algorithm.</p>

<div class="highlight highlight-source-r"><pre><span class="pl-smi">trainPC2</span> <span class="pl-k">&lt;-</span> <span class="pl-smi">trainPC</span>[ , <span class="pl-smi">selectedPCs</span>]
<span class="pl-smi">trainPC2</span> <span class="pl-k">&lt;-</span> cbind(<span class="pl-smi">trainPC2</span>, <span class="pl-s"><span class="pl-pds">'</span>classe<span class="pl-pds">'</span></span> <span class="pl-k">=</span> <span class="pl-smi">training_labels</span>)
<span class="pl-smi">modelFit</span> <span class="pl-k">&lt;-</span> randomForest(<span class="pl-smi">classe</span> <span class="pl-k">~</span> ., <span class="pl-v">data</span> <span class="pl-k">=</span> <span class="pl-smi">trainPC2</span>, <span class="pl-v">ntree</span> <span class="pl-k">=</span> <span class="pl-c1">50</span>)</pre></div>

<p>The out of sample error can now be estimated using the testing set that has not been used in the model selection or fitting.</p>

<div class="highlight highlight-source-r"><pre><span class="pl-c"># In Sample Error</span>
<span class="pl-smi">pred_training</span> <span class="pl-k">&lt;-</span> predict(<span class="pl-smi">modelFit</span>, <span class="pl-v">newdata</span> <span class="pl-k">=</span> <span class="pl-smi">trainPC</span>)
confusionMatrix(<span class="pl-smi">pred_training</span>, <span class="pl-smi">training_labels</span>)</pre></div>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 3906    0    0    0    0
##          B    0 2658    0    0    0
##          C    0    0 2396    0    0
##          D    0    0    0 2252    0
##          E    0    0    0    0 2525
## 
## Overall Statistics
##                                      
##                Accuracy : 1          
##                  95% CI : (0.9997, 1)
##     No Information Rate : 0.2843     
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  
##                                      
##                   Kappa : 1          
##  Mcnemar's Test P-Value : NA         
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            1.0000   1.0000   1.0000   1.0000   1.0000
## Specificity            1.0000   1.0000   1.0000   1.0000   1.0000
## Pos Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000
## Neg Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000
## Prevalence             0.2843   0.1935   0.1744   0.1639   0.1838
## Detection Rate         0.2843   0.1935   0.1744   0.1639   0.1838
## Detection Prevalence   0.2843   0.1935   0.1744   0.1639   0.1838
## Balanced Accuracy      1.0000   1.0000   1.0000   1.0000   1.0000
</code></pre>

<div class="highlight highlight-source-r"><pre><span class="pl-c"># Out of Sample Error</span>
<span class="pl-smi">pred_testing</span> <span class="pl-k">&lt;-</span> predict(<span class="pl-smi">modelFit</span>, <span class="pl-v">newdata</span> <span class="pl-k">=</span> <span class="pl-smi">testPC</span>)
confusionMatrix(<span class="pl-smi">pred_testing</span>, <span class="pl-smi">testing_labels</span>)</pre></div>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1615   34    9   14    4
##          B   14 1052   26    7   15
##          C   23   42  957   62   15
##          D   12    7   27  877    6
##          E   10    4    7    4 1042
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9419          
##                  95% CI : (0.9356, 0.9477)
##     No Information Rate : 0.2845          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9265          
##  Mcnemar's Test P-Value : 2.801e-06       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9648   0.9236   0.9327   0.9098   0.9630
## Specificity            0.9855   0.9869   0.9708   0.9894   0.9948
## Pos Pred Value         0.9636   0.9443   0.8708   0.9440   0.9766
## Neg Pred Value         0.9860   0.9818   0.9856   0.9824   0.9917
## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
## Detection Rate         0.2744   0.1788   0.1626   0.1490   0.1771
## Detection Prevalence   0.2848   0.1893   0.1867   0.1579   0.1813
## Balanced Accuracy      0.9751   0.9553   0.9518   0.9496   0.9789
</code></pre>

<div class="highlight highlight-source-r"><pre><span class="pl-c"># Validation set</span>
<span class="pl-smi">pred_validation</span> <span class="pl-k">&lt;-</span> predict(<span class="pl-smi">modelFit</span>, <span class="pl-v">newdata</span> <span class="pl-k">=</span> <span class="pl-smi">validationPC</span>)
<span class="pl-smi">answers</span> <span class="pl-k">&lt;-</span> as.character(<span class="pl-smi">pred_validation</span>)</pre></div>

<p>The out of sample accuracy is 94.7% which gives an error rate very close to our expected error rate of 5%.  Finally, the validation set is run through the model and the predictions will be submitted for validation.</p>

<p>The success of this classification algorithm demonstrates that it is possible to use personal activity monitors to document not only "how much" a person performed an exercise activity, but also "how well" they did it.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/grace-pehl/MLworkout">Grace Pehl, PhD</a> is maintained by <a href="https://github.com/grace-pehl">grace-pehl</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
