{"name":"Grace Pehl, PhD","tagline":"Can Your Fitbit Tell When You're Slacking? Classification of Weightlifting Technique from Personal Activity Trackers","body":"Objective: Build a machine learning algorithm to predict activity quality from activity monitors\r\n\r\nCourse project for Practical Machine Learning, part of the Johns Hopkins Data Science Specialization\r\n\r\n==========================\r\n\r\n#### Data citation:  \r\nVelloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\r\n\r\n### Introduction\r\n\r\nThe dataset consists of 160 features from accelerometers on the arm, forearm, belt, and dumbbell of 6 participants who performed a series of weightlifting exercises both correctly and incorrectly. The subjects performed a set of 10 repetitions of unilateral dumbbell biceps curls in 5 different ways.  The class A set was done with proper technique. The class B set was done throwing the elbows forward; class C, lifting the dumbbell only halfway; class D, lowering the dumbbell only halfway; and class E, throwing the hips forward. \r\n\r\nThe complete dataset is split into a training dataset with 13737 observations, a testing dataset with 5885 observations, and a validation set with 20 observations. In the training set, there are no complete cases (observations with no missing values for any feature). Removing 100 features with more than 80% missing values and 7 identification features, leaves 52 features and 13737 out of 13737 complete cases.  The distribution of classes in the training set is: \r\n\r\n Class   Technique        Counts   Proportion \r\n ------  ---------------  -------  -----------\r\n   A       Proper Form      3906     0.28       \r\n   B       Elbows Forward   2658     0.19       \r\n   C       Halfway Up       2396     0.17       \r\n   D       Halfway Down     2252     0.16       \r\n   E       Hips Forward     2525     0.18       \r\n\r\n### Principal Component Analysis  \r\nIn order to reduce the dimensionality of the dataset (the number of features) and to speed computation time, I perform a principal component analysis on the training set.  The preProcess function will center and scale the features prior to PCA and principal components will be calculated to retain 95% of the variance within the data.\r\n\r\n```r\r\n# Perform Principal Component Analysis on the training set\r\npreProc <- preProcess(training[ , -53], method = \"pca\")\r\n# Apply the transform to all 3 datasets\r\ntrainPC <- predict(preProc, training[ , -53])\r\ntestPC  <- predict(preProc, testing[ , -53])\r\nvalidationPC <- predict(preProc, validation[ , -53])\r\n```\r\nPCA reduces the number of features from 52 to 25.  Using the same components, I also transform the testing and validation sets.  \r\n\r\n```r\r\nsuppressMessages(library(randomForest))\r\nmodFit <- rfcv(trainPC, training_labels, step = 0.8, cv.fold = 10)\r\n```\r\n### Estimating Out of Sample Error with Cross Validation  \r\nI use random forest with 10-fold cross validation to select the most important features for the model.  In each fold, I retain the top 80% of features.\r\n\r\n![](analysis_files/figure-html/SelectingFeaturesPlot-1.png) \r\n\r\nThe cross validation results show that there is very little difference between using all 26 principal components and using only 11.  With 11 features, we would expect an out-of-sample error rate around 5%.  I use the the top 11 principal components (based on mean Gini decrease) to fit a random forest algorithm.\r\n\r\n```r\r\ntrainPC2 <- trainPC[ , selectedPCs]\r\ntrainPC2 <- cbind(trainPC2, 'classe' = training_labels)\r\nmodelFit <- randomForest(classe ~ ., data = trainPC2, ntree = 50)\r\n```\r\nThe out of sample error can now be estimated using the testing set that has not been used in the model selection or fitting.\r\n\r\n```r\r\n# In Sample Error\r\npred_training <- predict(modelFit, newdata = trainPC)\r\nconfusionMatrix(pred_training, training_labels)\r\n```\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 3906    0    0    0    0\r\n##          B    0 2658    0    0    0\r\n##          C    0    0 2396    0    0\r\n##          D    0    0    0 2252    0\r\n##          E    0    0    0    0 2525\r\n## \r\n## Overall Statistics\r\n##                                      \r\n##                Accuracy : 1          \r\n##                  95% CI : (0.9997, 1)\r\n##     No Information Rate : 0.2843     \r\n##     P-Value [Acc > NIR] : < 2.2e-16  \r\n##                                      \r\n##                   Kappa : 1          \r\n##  Mcnemar's Test P-Value : NA         \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Specificity            1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Pos Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Neg Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000\r\n## Prevalence             0.2843   0.1935   0.1744   0.1639   0.1838\r\n## Detection Rate         0.2843   0.1935   0.1744   0.1639   0.1838\r\n## Detection Prevalence   0.2843   0.1935   0.1744   0.1639   0.1838\r\n## Balanced Accuracy      1.0000   1.0000   1.0000   1.0000   1.0000\r\n```\r\n```r\r\n# Out of Sample Error\r\npred_testing <- predict(modelFit, newdata = testPC)\r\nconfusionMatrix(pred_testing, testing_labels)\r\n```\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1615   34    9   14    4\r\n##          B   14 1052   26    7   15\r\n##          C   23   42  957   62   15\r\n##          D   12    7   27  877    6\r\n##          E   10    4    7    4 1042\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9419          \r\n##                  95% CI : (0.9356, 0.9477)\r\n##     No Information Rate : 0.2845          \r\n##     P-Value [Acc > NIR] : < 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9265          \r\n##  Mcnemar's Test P-Value : 2.801e-06       \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9648   0.9236   0.9327   0.9098   0.9630\r\n## Specificity            0.9855   0.9869   0.9708   0.9894   0.9948\r\n## Pos Pred Value         0.9636   0.9443   0.8708   0.9440   0.9766\r\n## Neg Pred Value         0.9860   0.9818   0.9856   0.9824   0.9917\r\n## Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839\r\n## Detection Rate         0.2744   0.1788   0.1626   0.1490   0.1771\r\n## Detection Prevalence   0.2848   0.1893   0.1867   0.1579   0.1813\r\n## Balanced Accuracy      0.9751   0.9553   0.9518   0.9496   0.9789\r\n```\r\n```r\r\n# Validation set\r\npred_validation <- predict(modelFit, newdata = validationPC)\r\nanswers <- as.character(pred_validation)\r\n```\r\nThe out of sample accuracy is 94.7% which gives an error rate very close to our expected error rate of 5%.  Finally, the validation set is run through the model and the predictions will be submitted for validation.\r\n\r\nThe success of this classification algorithm demonstrates that it is possible to use personal activity monitors to document not only \"how much\" a person performed an exercise activity, but also \"how well\" they did it.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}